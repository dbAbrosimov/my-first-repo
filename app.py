import streamlit as st
from io import BytesIO
import io
from lxml import etree
from datetime import datetime, timedelta
from collections import defaultdict
import scipy.stats as ss
import plotly.express as px
import numpy as np
import logging
import traceback
import statistics
import pandas as pd
import re
logging.basicConfig(level=logging.DEBUG)

import csv

import io

# Expand page width
st.set_page_config(layout="wide")

# Sidebar: upload group mapping CSV
mapping_upload = st.sidebar.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ mapping CSV (Metric_to_Group_Mapping.csv)",
    type="csv", key="mapping"
)
# Load static mapping from uploaded file or fallback
group_mapping = {}
if mapping_upload is not None:
    reader = csv.DictReader(io.StringIO(mapping_upload.getvalue().decode('utf-8')))
    for row in reader:
        group_mapping[row['Metric']] = row['Group']
else:
    st.sidebar.warning("–§–∞–π–ª mapping CSV –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω ‚Äî –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ–ø–∞–¥—É—Ç –≤ 'other'.")

def get_group(metric_pretty_name):
    return group_mapping.get(metric_pretty_name, 'other')


# buffer for in-app logs
log_buffer = io.StringIO()
buffer_handler = logging.StreamHandler(log_buffer)
buffer_handler.setLevel(logging.DEBUG)
logging.getLogger().addHandler(buffer_handler)

# list for collecting debug messages
log_messages = []


# 1. Load raw XML into a DataFrame of (date, metric, value)
def load_data(file_bytes):
    records = []
    for _, elem in etree.iterparse(BytesIO(file_bytes), tag='Record'):
        t = elem.get('type')
        start = elem.get('startDate')
        if not t or not start:
            elem.clear(); continue
        dt = datetime.fromisoformat(start).date()
        if t.endswith('SleepAnalysis'):
            if elem.get('value') != 'HKCategoryValueSleepAnalysisAsleepDeep':
                elem.clear(); continue
            val = (datetime.fromisoformat(elem.get('endDate')) - datetime.fromisoformat(start)).total_seconds() / 60
            records.append({'date': dt, 'metric': t, 'value': val})
            # record bedtime (startDate hour+minute) as a separate metric
            dt_start = datetime.fromisoformat(start)
            bedtime_val = dt_start.hour * 60 + dt_start.minute
            records.append({'date': dt_start.date(), 'metric': 'Bedtime', 'value': bedtime_val})
            # record wake-up time (endDate hour+minute) as a separate metric
            dt_end = datetime.fromisoformat(elem.get('endDate'))
            wake_val = dt_end.hour * 60 + dt_end.minute
            records.append({'date': dt_end.date(), 'metric': 'WakeTime', 'value': wake_val})
        else:
            try:
                val = float(elem.get('value') or 0)
            except:
                val = 0
            records.append({'date': dt, 'metric': t, 'value': val})
        elem.clear()
    return pd.DataFrame(records)

# 2. Prepare pivot table aggregated by period
from pandas import Grouper

def prepare_table(df, period):
    # map period levels
    if period == '–ù–µ–¥–µ–ª—è':
        df['period'] = df['date'] - pd.to_timedelta(df['date'].dt.weekday, unit='D')
    elif period == '–ú–µ—Å—è—Ü':
        df['period'] = df['date'].values.astype('datetime64[M]')
    else:
        df['period'] = df['date']
    # metrics to average per period (sleep depths, body stats, variability)
    mean_pattern = 'SleepAnalysis|Weight|BMI|Variability|HRV'
    metric_means = df[df['metric'].str.contains(mean_pattern)]
    metric_sums  = df[~df['metric'].str.contains(mean_pattern)]
    pt_sum = metric_sums.pivot_table(
        index='period', columns='metric', values='value', aggfunc='sum')
    pt_mean = metric_means.pivot_table(
        index='period', columns='metric', values='value', aggfunc='mean')
    return pd.concat([pt_sum, pt_mean], axis=1).sort_index()

# 3. Analyze all pairs vectorized
from itertools import combinations

def analyze_pairs(wide_df, p_thr, min_N):
    results = []
    metrics = wide_df.columns.dropna()
    pairs = list(combinations(metrics, 2))
    progress_bar = st.progress(0, text="–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä—ã –º–µ—Ç—Ä–∏–∫...")
    for idx, (X, Y) in enumerate(pairs):
        series = wide_df[[X, Y]].dropna()
        N = len(series)
        if N < min_N:
            progress_bar.progress((idx + 1) / len(pairs))
            continue
        # skip if no variation in X or Y
        if series[X].nunique() < 2 or series[Y].nunique() < 2:
            progress_bar.progress((idx + 1) / len(pairs))
            continue
        try:
            res = ss.linregress(series[X], series[Y])
        except ValueError:
            progress_bar.progress((idx + 1) / len(pairs))
            continue
        if res.pvalue > p_thr:
            progress_bar.progress((idx + 1) / len(pairs))
            continue
        results.append({'X': X, 'Y': Y, 'r': res.rvalue, 'p': res.pvalue, 'N': N})
        progress_bar.progress((idx + 1) / len(pairs))
    progress_bar.empty()
    return pd.DataFrame(results).sort_values('p')

@st.cache_data(show_spinner=False)
def get_types(file_bytes):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤ <Record> –≤ —Ñ–∞–π–ª–µ.
    """
    types = set()
    for _, elem in etree.iterparse(BytesIO(file_bytes), tag='Record'):
        t = elem.get('type')
        if t:
            types.add(t)
        elem.clear()
    return sorted(types)

st.title("Health XML: –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ—Ç—Ä–∏–∫")

uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Apple Health XML", type="xml")
if not uploaded:
    st.stop()
file_bytes = uploaded.read()

types = get_types(file_bytes)


st.sidebar.markdown("### –ú–µ—Ç—Ä–∏–∫–∞ X (–¥–µ–Ω—å)")
x_type = st.sidebar.selectbox(
    "–û—Å—å X", types,
    index=types.index("HKQuantityTypeIdentifierStepCount") if "HKQuantityTypeIdentifierStepCount" in types else 0
)

st.sidebar.markdown("### –ú–µ—Ç—Ä–∏–∫–∞ Y (–Ω–æ—á—å)")
y_type = st.sidebar.selectbox(
    "–û—Å—å Y", types,
    index=types.index("HKCategoryTypeIdentifierSleepAnalysis") if "HKCategoryTypeIdentifierSleepAnalysis" in types else 0
)

# –í—ã–±–æ—Ä –ø–µ—Ä–∏–æ–¥–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
period = st.sidebar.selectbox('–ì—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å', ['–î–µ–Ω—å', '–ù–µ–¥–µ–ª—è', '–ú–µ—Å—è—Ü'])
# –ü–æ—Ä–æ–≥ p-value
p_thr = st.sidebar.slider('–ü–æ—Ä–æ–≥ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ p-value', min_value=0.01, max_value=0.1, value=0.05, step=0.01)

# Option to exclude trivial same-group pairs
drop_same = st.sidebar.checkbox('–ò—Å–∫–ª—é—á–∏—Ç—å –ø–∞—Ä—ã –∏–∑ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã', value=True)

# Vectorized workflow
raw_df = load_data(file_bytes)
wide_df = prepare_table(raw_df, period)
# determine min_N threshold: median N across all pairs or 10
all_counts = []
for X, Y in combinations(wide_df.columns, 2):
    all_counts.append(len(wide_df[[X, Y]].dropna()))
min_N = max(10, int(pd.Series(all_counts).median()))
pairs_df = analyze_pairs(wide_df, p_thr, min_N)

# human-readable metric name
def pretty(name):
    # remove HealthKit prefix
    base = re.sub(r'^HK(?:Category|Quantity)TypeIdentifier', '', name)
    # split camel case
    return re.sub(r'(?<!^)(?=[A-Z])', ' ', base).strip()


if not pairs_df.empty:
    df2 = pairs_df.copy()
    # compute ŒîY and –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ X for each pair
    delta_list = []
    optx_list = []
    for _, row in df2.iterrows():
        X, Y = row['X'], row['Y']
        # original metric names
        orig_X = pairs_df.loc[_,'X']
        orig_Y = pairs_df.loc[_,'Y']
        series = wide_df[[orig_X, orig_Y]].dropna()
        # ŒîY: average period-to-period change
        dy = series[orig_Y].diff().dropna()
        delta_list.append(dy.mean())
        # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ X: minimal X where Y>=95% of its max
        thresh = 0.95 * series[orig_Y].max()
        xs = series[orig_X][series[orig_Y] >= thresh]
        optx_list.append(xs.min() if not xs.empty else None)
    # assign
    df2['ŒîY'] = delta_list
    df2['OptX'] = optx_list

    # preserve original columns before mapping pretty names
    df2['X_raw'] = df2['X']
    df2['Y_raw'] = df2['Y']
    df2['X'] = df2['X'].map(pretty)
    df2['Y'] = df2['Y'].map(pretty)
    # format p-value to three decimal places
    def fmt_p(p):
        # format p-value to three decimal places
        return f"{p:.3f}"
    df2['p'] = df2['p'].apply(fmt_p)

    if drop_same:
        # keep only pairs where groups differ (using pretty names)
        df2 = df2[df2.apply(lambda row: get_group(row['X']) != get_group(row['Y']), axis=1)].reset_index(drop=True)

    # –ü–æ–∏—Å–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ
    search = st.text_input('üîç –ü–æ–∏—Å–∫ –º–µ—Ç—Ä–∏–∫ (X –∏–ª–∏ Y)', '')
    if search:
        mask = df2['X'].str.contains(search, case=False) | df2['Y'].str.contains(search, case=False)
        df_display = df2.loc[mask]
    else:
        df_display = df2
    st.dataframe(df_display[['X','Y','r','p','ŒîY','OptX','N']])

    # –¢–æ–ø –º–µ–∂–≥—Ä—É–ø–ø–æ–≤—ã—Ö —Å–≤—è–∑–µ–π
    if st.sidebar.checkbox('–ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ø-5 –º–µ–∂–≥—Ä—É–ø–ø–æ–≤—ã—Ö —Å–≤—è–∑–µ–π –ø–æ |r|', value=False):
        # —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–∞—Ä—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø –ø–æ pretty names
        cross_mask = []
        for _, row in df2.iterrows():
            if get_group(row['X']) != get_group(row['Y']):
                cross_mask.append(True)
            else:
                cross_mask.append(False)
        cross_df = df2.loc[cross_mask].copy()
        # —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É r –∏ –±–µ—Ä—ë–º —Ç–æ–ø-5
        cross_df['abs_r'] = cross_df['r'].abs()
        top5 = cross_df.sort_values('abs_r', ascending=False).head(5)
        st.subheader('–¢–æ–ø-5 —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –≥—Ä—É–ø–ø–∞–º–∏')
        st.dataframe(top5[['X','Y','r','p','N','ŒîY','OptX']], use_container_width=True)
else:
    st.write("–ù–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Å–≤—è–∑–µ–π –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ p-value.")

# Interactive description per row
with st.expander("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ"):
    selected_row_index = st.number_input(
        "–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞",
        min_value=0,
        max_value=len(df2) - 1 if not pairs_df.empty else 0,
        step=1,
        value=0
    )
    if not pairs_df.empty and 0 <= selected_row_index < len(df2):
        row = df2.iloc[selected_row_index]
        st.write(f"**–ü–∞—Ä–∞–º–µ—Ç—Ä X:** {row['X']} (–æ—Ä–∏–≥–∏–Ω–∞–ª: {row['X_raw']})")
        st.write(f"**–ü–∞—Ä–∞–º–µ—Ç—Ä Y:** {row['Y']} (–æ—Ä–∏–≥–∏–Ω–∞–ª: {row['Y_raw']})")
        st.write(f"**–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (r):** {row['r']:.3f}")
        st.write(f"**p-–∑–Ω–∞—á–µ–Ω–∏–µ:** {row['p']}")
        st.write(f"**–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π (N):** {row['N']}")
        st.write(f"**–°—Ä–µ–¥–Ω–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ ŒîY:** {row['ŒîY']:.3f}")
        st.write(f"**–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ X (–≥–¥–µ Y >= 95% max):** {row['OptX']}")
        # Plot scatter
        orig_X = row['X_raw']
        orig_Y = row['Y_raw']
        series = wide_df[[orig_X, orig_Y]].dropna()
        # Compute and display average and expected benefit
        mean_x = series[orig_X].mean()
        mean_y = series[orig_Y].mean()
        threshold = 0.95 * series[orig_Y].max()
        expected_benefit = threshold - mean_y
        st.write(f"–°–µ–π—á–∞—Å –≤–∞—à —Å—Ä–µ–¥–Ω–∏–π **{row['X']}** ‚âà {mean_x:.1f}.")
        st.write(f"–ü—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –¥–æ **{row['OptX']:.1f}** –≤–∞—à —Å—Ä–µ–¥–Ω–∏–π **{row['Y']}** –º–æ–∂–µ—Ç –≤—ã—Ä–∞—Å—Ç–∏ –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ {expected_benefit:.1f}.")
        fig = px.scatter(series, x=orig_X, y=orig_Y,
                         title=f"–ì—Ä–∞—Ñ–∏–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ {row['Y']} –æ—Ç {row['X']}",
                         labels={orig_X: row['X'], orig_Y: row['Y']})
        st.plotly_chart(fig, use_container_width=True)
